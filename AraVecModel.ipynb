{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aravec Model Word Embedding\n",
    "* github model source link : https://github.com/bakrianoo/aravec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDX5kzUJJp8B"
   },
   "source": [
    "## Install/Load the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re ##regularexpression\n",
    "import spacy\n",
    "import nltk\n",
    "import util as ut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils  # utility fnc for pickling, common scipy operations etc\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = gensim.models.Word2Vec.load(\n",
    " #   \"C:/Users/PC/Desktop/ALL/II3IA/NLP/full_grams_cbow_100_twitter.mdl\")\n",
    "#model.wv.save_word2vec_format(\"./spacyModel/aravec.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"./spacy.aravec.model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing Class\n",
    "class Preprocessor:\n",
    "    def __init__(self, tokenizer, **cfg):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, text):\n",
    "        preprocessed = ut.clean_str(text)\n",
    "        return self.tokenizer(preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the `Preprocessor` Class\n",
    "nlp.tokenizer = Preprocessor(nlp.tokenizer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"طبخ الطباخ المرطبات\"\n",
    "text11 = \"طبخ الطباخ المركبات\"\n",
    "text2 = \"حقق منتخبنا الوطني الاول لكرة القدم كل اهدافه المطلوبة\"\n",
    "text22 = \"حقق منتخبنا الوطني الاول لكرة القسم كل اهدافه المطلوبة\"\n",
    "text3 = \"تنطلق اليوم الدورة البرامجية الجديدة للتليفزيون والاذاعة و برنامج الشباب و التي تستمر طوال اشهر ابريل و مايو و يونيو\"\n",
    "text33 = \"تنطلق اليوم الدورة البرامجية الجديدة للتليفزيون و الاشاعة و برنامج الشباب و التي تستمر طوال اشهر ابريل و مايو و يونيو\"\n",
    "text4 = \"منتخبنا يتألق ويهم الشباك\"\n",
    "text44 = \"منتخبنا يتألق ويهز الشباك\"\n",
    "text5 =\"اصطاد الصياد سمكة\"\n",
    "text55 =\"اصطاد الصياد سكة\"\n",
    "text6 =\"لكن قد لا يعرف الكثيرون أن موهبتها التمثيلية وجدت طريقها إلى عالم الفن بالصدفة البحته\"\n",
    "text66 = \"لكن قد لا يعرف الكثيرون أن موهبتها التمثيلية وجدت طريقها إلى عالم الفن بالصدفة البحته\"\n",
    "text7 = \"شهد السباق تفاعلا وحضورا كبيرا من محبي هذه الرياضة الاصيلة كونها تمثل رمزا للاصالة العربية ونهجا على طريق الاجداد\"\n",
    "text77 = \"شهد السباق تفاعلا وحضورا كبيرا من محبي هذه الرياضة الاصيلة كونها تمثل رمزا للاصالة العبرية ونهجا على طريق الاجداد\"\n",
    "text8=\"\"\n",
    "text = text77\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Least Similar vector\n",
    "* odd word is the one that is the least similar to all other words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "طبخ  =  0.5630887768772145\n",
      "طباخ  =  0.5994759343985119\n",
      "مركب  =  0.45359232925455717\n",
      "score de chaque mot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'المركبات'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def min_max_similarity(text):\n",
    "    origin = nltk.word_tokenize((ut.very_clean(text)))  # preprocessing\n",
    "    text = nltk.word_tokenize(ut.lemmatize(ut.very_clean(text))) #lemmatizing\n",
    "    token = [nlp(x) for x in text] #embedding\n",
    "    avg_list=[]\n",
    "    for i in range(len(token)): \n",
    "        sim_list=[]\n",
    "        for j in range(len(token)):\n",
    "            sim_list.append(token[i].similarity(token[j]))\n",
    "        avg_list.append(np.mean(sim_list))\n",
    "        #print(token[i], \" = \", avg_list[i])\n",
    "    print(\"score de chaque mot\")\n",
    "    return(origin[avg_list.index(min(avg_list))])\n",
    "min_max_similarity(text11)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Furthest Vector\n",
    "* odd word is the word whose vector is the furthest(considering cosine similarity as distance) from the context vector of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'المركبات'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Un mot est considéré erroné si son vecteur est éloigné \n",
    "# (par rapport à un seuil) du vecteur moyenne/somme de son contexte \n",
    "# context ~ (vecteur moyenne des autres mots)\n",
    "\n",
    "def context_vec(token, word):\n",
    "    avg = np.mean([nlp(w).vector for w in token if w != word],\n",
    "                  axis=0)  # avg euclidean distance\n",
    "    return avg\n",
    "\n",
    "#-----------------------\n",
    "#distance(similarity) between two vectors\n",
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity=0\n",
    "    cosine_similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
    "    return cosine_similarity\n",
    "#---------------------------------------\n",
    "\n",
    "\n",
    "def odd_word(text):\n",
    "    origin = nltk.word_tokenize((ut.very_clean(text))) #preprocessing\n",
    "    text = nltk.word_tokenize(ut.lemmatize(ut.very_clean(text))) #lemmatizing\n",
    "    sptext = [nlp(x) for x in text] #embedding\n",
    "    l = list([])\n",
    "    for word in sptext:\n",
    "        context = context_vec(sptext, word)\n",
    "        #l = [math.dist(nlp(w).vector,context) for w in token]\n",
    "        l.append(similarity_cosine(nlp(word).vector, context))\n",
    "    farthestword = origin[l.index(min(l))]\n",
    "    return(farthestword)\n",
    "\n",
    "\n",
    "odd_word(text11)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesure de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6c8f846148a3e4d140e6ddf63c190cff559dcf260a4a21539f0978f2b58638c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
