{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "* dataset= https://sourceforge.net/projects/kalimat/files/kalimat/document-collection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util as ut"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_text1 = \"تنطلق اليوم الدورة البرامجية الجديدة للتليفزيون والاذاعة و برنامج الشباب و التي تستمر طوال اشهر ابريل و مايو و يونيو\"\n",
    "err_text1 = \"تنطلق اليوم الدورة البرامجية الجديدة للتليفزيون و الاشاعة و برنامج الشباب و التي تستمر طوال اشهر ابريل و مايو و يونيو\"\n",
    "\n",
    "test_text2 = \"حقق منتخبنا الوطني الاول لكرة القدم كل اهدافه المطلوبة\"\n",
    "err_text2 = \"حقق منتخبنا الوطني الاول لكرة القسم كل اهدافه المطلوبة\"\n",
    "err_text2 = \"منتخبنا يتألق ويهم الشباك\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using bigger corpus\n",
    "def CheckNGramInCorpus(texttoken,corpustoken): #return ngram that don't exist in corpustoken\n",
    "    ngram = [g for g in texttoken if g not in corpustoken]\n",
    "    return (ngram)\n",
    "\n",
    "def WrongNGram(text):\n",
    "    token = ut.get_all_ngrams(text, nrange=2)\n",
    "    ngram_i = []\n",
    "    for directory in [\"articlesCulture\",\"articlesInternational\",\"articlesSports\"]:\n",
    "        for filename in os.listdir(\"C:/Users/PC/Desktop/ALL/II3IA/NLP/articles/\"+directory):\n",
    "            f = open(\"C:/Users/PC/Desktop/ALL/II3IA/NLP/articles/\" +\n",
    "                     directory+\"/\"+filename, \"r\", encoding=\"utf8\")\n",
    "            ngram_i = ut.get_all_ngrams(f.read().replace('\\n',' '), nrange=2) \n",
    "            #print(token)\n",
    "            token = CheckNGramInCorpus(token,ngram_i)\n",
    "            if len(token)==0:\n",
    "                print(token ,'no error')\n",
    "                return(token)\n",
    "            f.close()        \n",
    "    print(\"potential error in : \\n \",token)\n",
    "    return(token)\n",
    "                    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improvements 1: \n",
    "add cleaning, lemmatizing + removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "#! pip install qalsadi\n",
    "import qalsadi.lemmatizer\n",
    "lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "def lemmatize(text):\n",
    "    t=lemmer.lemmatize_text(text)\n",
    "    s = ut.stringify(t)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming (meth non utilisée)\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def stemmatize(text):\n",
    "    tokens = text.split()\n",
    "    stem_tokens = [stemmer.suf32(token) for token in tokens]\n",
    "    s= ut.stringify(stem_tokens)\n",
    "    return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckNGramInCorpus(texttoken,corpustoken): #return ngram that don't exist in corpustoken\n",
    "    ngram = [g for g in texttoken if g not in corpustoken]\n",
    "    return (ngram)\n",
    "def WrongNGramV2(text,n = 2):\n",
    "    text = ut.very_clean(text)\n",
    "    text = lemmatize(text)\n",
    "    token = ut.get_all_ngrams(text,nrange = n)\n",
    "    ngram_i = []\n",
    "    for directory in [\"articlesCulture\", \"articlesInternational\", \"articlesSports\"]:\n",
    "        for filename in os.listdir(\"C:/Users/PC/Desktop/ALL/II3IA/NLP/articles/\"+directory):\n",
    "            f = open(\"C:/Users/PC/Desktop/ALL/II3IA/NLP/articles/\" +\n",
    "                     directory+\"/\"+filename, \"r\", encoding=\"utf8\")\n",
    "            g = ut.very_clean(f.read().replace('\\n', ' '))\n",
    "            g = lemmatize(g)\n",
    "            ngram_i = ut.get_all_ngrams(g,nrange=n)\n",
    "            #print(token)\n",
    "            token = CheckNGramInCorpus(token,ngram_i)\n",
    "            if len(token)==0:\n",
    "                #print(token ,'no error')\n",
    "                return(token)\n",
    "            f.close()        \n",
    "    #print(\"potential error in : \\n \",token)\n",
    "    return(token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements 2 : \n",
    "(la meth precedente est tres lourde : parcourir tous le fichier pour prouver que le ngram n'existe pas)\n",
    "--> solution : sortir de la boucle si on retrouve un ngram tres proche (inversion/suppression/ajout d'un caratere) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return ngram that don't exist in corpustoken or exist with minor change (typography mistake)\n",
    "def OneSubstitution(word, word2):# return true if exists one and only onesubstituion error\n",
    "    if len(word)==len(word2):\n",
    "        l = []\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == word2[i]:\n",
    "                l.append(0)\n",
    "            else:\n",
    "                l.append(1)\n",
    "        return (sum(l) == 1)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "\n",
    "def OneInversion(word, word2):\n",
    "    if len(word) == len(word2):  # returns true if exists one and only inversion error\n",
    "        l1 = []\n",
    "        l2 = []\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == word2[i]:\n",
    "                l1.append(0)\n",
    "            else:\n",
    "                l1.append(1)\n",
    "                l2.append(word[i])\n",
    "                l2.append(word2[i])\n",
    "        if sum(l1) == 2:\n",
    "          return(l2[0] == l2[3] and l2[1] == l2[2])\n",
    "        else:\n",
    "            return(False)\n",
    "    else:\n",
    "        return(False)\n",
    "\n",
    "\n",
    "def AddSubErr(word1, word2):  # ajout/suppression d'un seul caracter\n",
    "    w1 = set(word1)\n",
    "    w2 = set(word2)\n",
    "    maxi=max(len(w1-w2),len(w2-w1))\n",
    "    if maxi != 1:\n",
    "        return(False)\n",
    "    else :\n",
    "        if len(w1)-len(w2) == 1:\n",
    "            lw1=list(word1)\n",
    "            lw1.remove(''.join(w1-w2))\n",
    "            s = ''.join(str(x) for x in lw1)\n",
    "            return(s==word2)\n",
    "        elif len(w1)-len(w2) == -1:\n",
    "            lw2 = list(word2)\n",
    "            lw2.remove(''.join(w2-w1))\n",
    "            s = ''.join(str(x) for x in lw2)\n",
    "            return(s == word1)\n",
    "        else:\n",
    "            return(False)\n",
    "def lookslike(words, corpustoken):\n",
    "    for word in words:\n",
    "        for c in corpustoken:\n",
    "            if AddSubErr(word,c): \n",
    "                return(word)\n",
    "            elif OneInversion(word,c) or OneSubstitution(word,c) : #inversion / substitution\n",
    "                return(word)\n",
    "    return('')\n",
    "\n",
    "def CheckNGramInCorpus(texttoken, corpustoken):\n",
    "    ngram = [g for g in texttoken if g not in corpustoken]\n",
    "    return (ngram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WrongNGramV3(text, n=2):\n",
    "    text = ut.very_clean(text)\n",
    "    #text = lemmatize(text)\n",
    "    token = ut.get_all_ngrams(text, nrange=n)\n",
    "    ngram_i = []\n",
    "    err = ''\n",
    "    for directory in [\"articlesSports\",\"articlesCulture\", \"articlesInternational\"]:\n",
    "        for filename in os.listdir(\"C:/Users/PC/Desktop/ALL/II3IA/NLP/articles/\"+directory):\n",
    "            f = open(\"C:/Users/PC/Desktop/ALL/II3IA/NLP/articles/\" +\n",
    "                     directory+\"/\"+filename, \"r\", encoding=\"utf8\")\n",
    "            g = ut.very_clean(f.read().replace('\\n', ' '))\n",
    "            #g = lemmatize(g)\n",
    "            ngram_i = ut.get_all_ngrams(g, nrange=n)\n",
    "            #print(token)\n",
    "            #print(ngram_i)\n",
    "            token = CheckNGramInCorpus(token, ngram_i)\n",
    "            err = lookslike(token,ngram_i)\n",
    "            if len(err) !=0 :\n",
    "                print(\"potential error in: \",err) \n",
    "            if len(token) == 0:\n",
    "                print(token ,'no error')\n",
    "                return(token)\n",
    "            f.close()\n",
    "    print(\"potential error in :\", err)\n",
    "    print(\"error in : \\n \",token)\n",
    "    return(token)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mispelled_word (text,n=2): #sans considerer lemmatizationb\n",
    "    #l = WrongNGram(text)\n",
    "    #l = WrongNGramV2(text)\n",
    "    l = WrongNGramV3(text)\n",
    "    print(l)\n",
    "    if len(l)>1:\n",
    "        l2 = l[0].split(\"_\")\n",
    "        print(\"mispelled word is : \")\n",
    "        return(l2[-1])\n",
    "    elif len(l) == 1 : #mistake in first or last word\n",
    "        print(\"the error is either first or last word\")\n",
    "        return(l)\n",
    "    else :\n",
    "        print(\"no error in this sentence\")\n",
    "        return(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'err_text2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\Desktop\\ALL\\II3IA\\NLP\\aravec\\aravec-master\\LanguageModeling.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/Desktop/ALL/II3IA/NLP/aravec/aravec-master/LanguageModeling.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m WrongNGramV3(err_text2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'err_text2' is not defined"
     ]
    }
   ],
   "source": [
    "WrongNGramV3(err_text2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combinaison : modele de language avec model AraVec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-grams avec model AraVec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = gensim.models.Word2Vec.load(\n",
    "    \"C:/Users/PC/Desktop/ALL/II3IA/NLP/full_grams_cbow_100_twitter.mdl\")\n",
    "model.wv.save_word2vec_format(\"./spacyModel/aravec.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text1 = \"لاتعطني سمكا بل علمني كيف اصطاد\"\n",
    "ut.get_all_ngrams(text1, nrange=4)\n",
    "\n",
    "print(\"n-grams existing in vocabulary\")\n",
    "for token in ut.get_all_ngrams(text1, nrange=4):\n",
    "    if token in model.wv:\n",
    "        print(token)\n",
    "        # only one bigram exists\n",
    "print(\"words existing in vocabulary\")\n",
    "for token in text1.split():\n",
    "    if token in model.wv:\n",
    "        print(token)\n",
    "        # only one word exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6c8f846148a3e4d140e6ddf63c190cff559dcf260a4a21539f0978f2b58638c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
